{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7623d8",
   "metadata": {
    "id": "fe7623d8"
   },
   "source": [
    "## Problem Summary\n",
    "\n",
    "To replicate the Deep Structural Causal Models for Tractable Counterfactual Inference[1]paper , and apply it to google cartoon faces dataset[3] and answer counterfactual queries on the same. \n",
    "\n",
    "We aim to explicitly model causal relationships with a fully specified causal models with no unobserved confounding and inferring exogenous noise via  normalising flows.\n",
    "\n",
    "Our goal is to validate our causal assumptions; if our causal assumptions are valid, these simulations should align with our imagination. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "QxOfXsj8yPzP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QxOfXsj8yPzP",
    "outputId": "789b75cc-5674-47d8-f4cf-a1bb1d13cf6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |▌                               | 10 kB 30.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 20 kB 21.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 30 kB 17.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 40 kB 14.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 51 kB 7.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 61 kB 8.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 71 kB 8.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 81 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 92 kB 9.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 102 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 112 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 122 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 133 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 143 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 153 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 163 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 174 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 184 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 194 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 204 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 215 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 225 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 235 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 245 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 256 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 266 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 276 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 286 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 296 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 307 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 317 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 327 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 337 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 348 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 358 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 368 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 378 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 389 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 399 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 409 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 419 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 430 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 440 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 450 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 460 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 471 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 481 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 491 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 501 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 512 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 522 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 532 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 542 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 552 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 563 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 573 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 583 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 593 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 604 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 614 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 624 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 634 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 645 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 655 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 665 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 675 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 678 kB 7.1 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q torch pyro-ppl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2JtHzszgyjpi",
   "metadata": {
    "id": "2JtHzszgyjpi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1158126",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1158126",
    "outputId": "3427a85b-2cbd-4c1b-c95e-64c8b6ca7e25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "from pyro.distributions.conditional import ConditionalTransformModule\n",
    "from pyro.distributions.torch_transform import TransformModule\n",
    "from pyro.distributions import transforms as pyro_transforms\n",
    "from torch.distributions import transforms \n",
    "import torch\n",
    "from torch.distributions.utils import lazy_property\n",
    "from torch.distributions import constraints\n",
    "from torch.distributions.transforms import Transform\n",
    "import numpy as np\n",
    "\n",
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats \n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install pytorch-lightning==1.3.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint \n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "try:\n",
    "     set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "import torch \n",
    "import pyro.distributions as dist \n",
    "from torch.distributions.normal import Normal \n",
    "from torch.distributions.uniform import Uniform \n",
    "from torch.distributions.beta import Beta \n",
    "from scipy import stats \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from pyro.nn import PyroModule, pyro_method \n",
    "import torch\n",
    "import pyro\n",
    "from pyro.nn import PyroModule, pyro_method\n",
    "from pyro.distributions import Normal, TransformedDistribution\n",
    "from pyro.distributions.torch_transform import ComposeTransformModule\n",
    "from pyro.distributions.conditional import ConditionalTransformedDistribution\n",
    "from pyro.distributions.transforms import (\n",
    "    Spline, ExpTransform, ComposeTransform, ConditionalAffineCoupling,\n",
    "    GeneralizedChannelPermute, SigmoidTransform\n",
    "    )\n",
    "from pyro.distributions.transforms import ComposeTransform, SigmoidTransform, AffineTransform \n",
    "from pyro.nn import DenseNN \n",
    "import PIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hv-3w83qyp7A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hv-3w83qyp7A",
    "outputId": "1dd17c96-de5f-4860-c446-4de0b65a1a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "! cp -r /content/drive/My\\ Drive/data/cartoon-faces/filtered_data_500.npy ./ \n",
    "# ! cp -r /content/drive/My\\ Drive/data/cartoon-faces/cartoon_resized.npy ./ \n",
    "! cp -r /content/drive/My\\ Drive/data/cartoon-faces/filtered_data_500.csv ./ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a9e40",
   "metadata": {
    "id": "500a9e40"
   },
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f94d0",
   "metadata": {
    "id": "b63f94d0"
   },
   "source": [
    "We use the google cartoon dataset to train our model. The original 4D (10 k) dataset was transformed into lower dimension grayscale dataset due to computation resource limitation. \n",
    "\n",
    "Also, for simplicity we convert the categorical feature into binary (glasses/no glasses)\n",
    "\n",
    "![Dataset Snapshot](./images/cartoon_snapshot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f5eeb",
   "metadata": {
    "id": "4b6f5eeb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba35ca6b",
   "metadata": {
    "id": "ba35ca6b"
   },
   "outputs": [],
   "source": [
    "def load_data(path_1,path_2):\n",
    "    \"\"\"Open the folder for cartoon dataset and combine them into one dataset with added column flenames which stores\n",
    "    the corresponding png filename\n",
    "    :param path_1: File path to the cartoon dataset directory\n",
    "    :param path_2: File path to save the cobined csv file\n",
    "    :returns: saves the combined file to the given path_2\"\"\"\n",
    "\n",
    "    import os\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "\n",
    "    os.chdir(path_1)\n",
    "    #os.chdir(\"Documents/GitHub/causalfairness/cartoonset10k/\")\n",
    "    extension = 'csv'\n",
    "    all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "    # combine all files in the list\n",
    "    combined_csv = pd.DataFrame()\n",
    "    for f in all_filenames:\n",
    "        df = pd.read_csv(f)\n",
    "        df = df.T\n",
    "        df = df.reset_index()\n",
    "        df = df.drop([0,2])\n",
    "        df['filename'] = f\n",
    "        df = df.reset_index()\n",
    "        combined_csv = combined_csv.append(df,ignore_index=True)\n",
    "\n",
    "    # export to csv\n",
    "    combined_csv = combined_csv.drop(columns=['level_0'])\n",
    "    combined_csv.columns = ['eye_angle','eye_lashes','eye_lid','chin_length','eyebrow_weight','eyebrow_shape','eyebrow_thickness','face_shape','facial_hair','hair','eye_color','face_color','hair_color','glasses','glasses_color','eye_slant','eyebrow_width','eye_eyebrow_distance','filename']\n",
    "    combined_csv.to_csv(path_2+\"combined_csv.csv\", index=False, encoding='utf-8-sig')\n",
    "    return\n",
    "\n",
    "def columntobinary(path_1,path_2):\n",
    "    \"\"\"Open the combined data file and make a filtered binary datafile\n",
    "    :param path_1: File path to the combined file\n",
    "    :param path_2: File path to save binary file\n",
    "    :returns: saves the binary file to path_2\"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(path_1)\n",
    "    df[\"facial_hair\"].replace({14: 0}, inplace=True)\n",
    "    df[\"glasses\"].replace({11: 0}, inplace=True)\n",
    "    df.to_csv(path_2+\"filtered_data_binary_new.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d4958d",
   "metadata": {
    "id": "71d4958d"
   },
   "outputs": [],
   "source": [
    "def grayscale(arr): \n",
    "    arr = arr.astype(np.uint8) \n",
    "    arr = PIL.Image.fromarray(arr).resize(size=(64,64)).convert(\"L\")\n",
    "    arr = np.expand_dims(np.array(arr), axis=0) \n",
    "    return torch.Tensor(arr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "yjDD0Q9YzIM_",
   "metadata": {
    "id": "yjDD0Q9YzIM_"
   },
   "outputs": [],
   "source": [
    "cartoon_features = pd.read_csv(\"filtered_data_500.csv\") \n",
    "cartoon_features = cartoon_features.drop([\"filename\"], axis=1) \n",
    "cartoon_features = cartoon_features.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3206e527",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3206e527",
    "outputId": "86900e66-f2f7-4c4d-8ea2-76dd01184b1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "cartoon = np.load(\"filtered_data_500.npy\") \n",
    "cartoon_resized = list(map(lambda x: grayscale(x), cartoon)) \n",
    "cartoon_resized = np.concatenate(cartoon_resized, axis=0) \n",
    "\n",
    "cartoon_data = list(map(lambda x, y: (torch.Tensor(np.expand_dims(x, axis=0)).to(device), torch.Tensor(y).to(device)), cartoon_resized, cartoon_features))  \n",
    "\n",
    "pl.seed_everything(42) \n",
    "train_loader = data.DataLoader(cartoon_data, batch_size=256, shuffle=False, drop_last=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63ff51",
   "metadata": {
    "id": "1b63ff51"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace2ee1",
   "metadata": {
    "id": "9ace2ee1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c28fe327",
   "metadata": {
    "id": "c28fe327"
   },
   "source": [
    "## Model Pieces Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8bb82",
   "metadata": {
    "id": "49f8bb82"
   },
   "source": [
    "#### Normalizing Flows\n",
    "\n",
    "\n",
    "Normalizing flows are a family of methods which allows for constructing more flexible probability distributions, commonly learned using neural networks. \n",
    "\n",
    "The path traversed by the random variables is the flow and the full chain formed by the successive distributions   is called a normalizing flow. \n",
    "\n",
    "Required by the computation in the equation, a transformation function  should satisfy two properties:\n",
    "\n",
    "    1. It is easily invertible.\n",
    "    2. Its Jacobian determinant is easy to compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b948b",
   "metadata": {
    "id": "921b948b"
   },
   "source": [
    "#### Normalizing Flows on Images\n",
    "\n",
    "Build a normalizing flow that maps an input image  to an equally sized latent space. \n",
    "\n",
    "Training and Validation: Perform density estimation in the forward direction by applying  a series of flow transformations on the input 𝑥 and estimate the probability of the input by determining the probability of the transformed point  𝑧  given a prior, and the change of volume caused by the transformations. \n",
    "\n",
    "Inference:  density estimation and sample new points by inverting the flow transformations\n",
    "\n",
    "\n",
    "![Normalizing flows on Images](./images/nf_images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4773e",
   "metadata": {
    "id": "a1b4773e"
   },
   "source": [
    "#### Coupling Layers\n",
    "\n",
    "A popular flow layer which lends itself to the architecture of neural networks is the coupling layer\n",
    "\n",
    "A given input z is split into two parts, the first part is passed through unchanged while the second part has a function dependent on both parts applied to it\n",
    "\n",
    "A standard version is the affine transformation, implemented as: $z_{1:j}'' = z_{1:j}, z_{j+1,d}' = \\mu(z_{1:j}) + \\sigma(z_{1:j})*z_{j+1:d}$ \n",
    "\n",
    "![Coupling Layers](./images/coupling_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff59fb",
   "metadata": {
    "id": "c5ff59fb"
   },
   "source": [
    "### Custom transforms (not available in torch or pyro) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d1396",
   "metadata": {},
   "source": [
    "### Squeeze Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f605a8d",
   "metadata": {
    "id": "5f605a8d"
   },
   "outputs": [],
   "source": [
    "class SqueezeTransform(Transform):\n",
    "    \"\"\"A transformation defined for image data that trades spatial dimensions for channel\n",
    "    dimensions, i.e. \"squeezes\" the inputs along the channel dimensions.\n",
    "    Implementation adapted from https://github.com/pclucas14/pytorch-glow and\n",
    "    https://github.com/chaiyujin/glow-pytorch.\n",
    "    Reference:\n",
    "    > L. Dinh et al., Density estimation using Real NVP, ICLR 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    domain = constraints.real\n",
    "    codomain = constraints.real\n",
    "    bijective = True\n",
    "    event_dim = 3\n",
    "    volume_preserving = True\n",
    "\n",
    "    def __init__(self, factor=2):\n",
    "        super().__init__(cache_size=1)\n",
    "\n",
    "        self.factor = factor\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        \"\"\"\n",
    "        :param x: the input into the bijection\n",
    "        :type x: torch.Tensor\n",
    "        Invokes the bijection x=>y; in the prototypical context of a\n",
    "        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n",
    "        the base distribution (or the output of a previous transform)\n",
    "        \"\"\"\n",
    "        if inputs.dim() < 3:\n",
    "            raise ValueError(f'Expecting inputs with at least 3 dimensions, got {inputs.shape} - {inputs.dim()}')\n",
    "\n",
    "        *batch_dims, c, h, w = inputs.size()\n",
    "        num_batch = len(batch_dims)\n",
    "\n",
    "        if h % self.factor != 0 or w % self.factor != 0:\n",
    "            raise ValueError('Input image size not compatible with the factor.')\n",
    "\n",
    "        inputs = inputs.view(*batch_dims, c, h // self.factor, self.factor, w // self.factor,\n",
    "                             self.factor)\n",
    "        permute = np.array((0, 2, 4, 1, 3)) + num_batch\n",
    "        inputs = inputs.permute(*np.arange(num_batch), *permute).contiguous()\n",
    "        inputs = inputs.view(*batch_dims, c * self.factor * self.factor, h // self.factor,\n",
    "                             w // self.factor)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _inverse(self, inputs):\n",
    "        \"\"\"\n",
    "        :param y: the output of the bijection\n",
    "        :type y: torch.Tensor\n",
    "        Inverts y => x.\n",
    "        \"\"\"\n",
    "        if inputs.dim() < 3:\n",
    "            raise ValueError(f'Expecting inputs with at least 3 dimensions, got {inputs.shape}')\n",
    "\n",
    "        *batch_dims, c, h, w = inputs.size()\n",
    "        num_batch = len(batch_dims)\n",
    "\n",
    "        if c < 4 or c % 4 != 0:\n",
    "            raise ValueError('Invalid number of channel dimensions.')\n",
    "\n",
    "        inputs = inputs.view(*batch_dims, c // self.factor ** 2, self.factor, self.factor, h, w)\n",
    "        permute = np.array((0, 3, 1, 4, 2)) + num_batch\n",
    "        inputs = inputs.permute(*np.arange(num_batch), *permute).contiguous()\n",
    "        inputs = inputs.view(*batch_dims, c // self.factor ** 2, h * self.factor, w * self.factor)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the elementwise determinant of the log Jacobian, i.e.\n",
    "        log(abs([dy_0/dx_0, ..., dy_{N-1}/dx_{N-1}])). Note that this type of\n",
    "        transform is not autoregressive, so the log Jacobian is not the sum of the\n",
    "        previous expression. However, it turns out it's always 0 (since the\n",
    "        determinant is -1 or +1), and so returning a vector of zeros works.\n",
    "        \"\"\"\n",
    "\n",
    "        log_abs_det_jacobian = torch.zeros(x.size()[:-3], dtype=x.dtype, layout=x.layout, device=x.device)\n",
    "        return log_abs_det_jacobian\n",
    "\n",
    "    def get_output_shape(self, c, h, w):\n",
    "        return (c * self.factor * self.factor,\n",
    "                h // self.factor,\n",
    "                w // self.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8909d96",
   "metadata": {},
   "source": [
    "#### Reshape Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeTransform(Transform):\n",
    "    domain = constraints.real\n",
    "    codomain = constraints.real\n",
    "    bijective = True\n",
    "    volume_preserving = True\n",
    "\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super().__init__()\n",
    "        # self.event_dim = len(input_shape)\n",
    "        self.inv_event_dim = len(output_shape)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        \"\"\"\n",
    "        :param x: the input into the bijection\n",
    "        :type x: torch.Tensor\n",
    "        Invokes the bijection x=>y; in the prototypical context of a\n",
    "        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n",
    "        the base distribution (or the output of a previous transform)\n",
    "        \"\"\"\n",
    "        batch_dims = inputs.shape[:-self.event_dim]\n",
    "        inp_shape = inputs.shape[-self.event_dim:]\n",
    "        if inp_shape != self.input_shape:\n",
    "            raise RuntimeError('Unexpected inputs shape ({}, but expecting {})'\n",
    "                               .format(inp_shape, self.input_shape))\n",
    "        return inputs.reshape(*batch_dims, *self.output_shape)\n",
    "\n",
    "    def _inverse(self, inputs):\n",
    "        \"\"\"\n",
    "        :param y: the output of the bijection\n",
    "        :type y: torch.Tensor\n",
    "        Inverts y => x.\n",
    "        \"\"\"\n",
    "        batch_dims = inputs.shape[:-self.inv_event_dim]\n",
    "        inp_shape = inputs.shape[-self.inv_event_dim:]\n",
    "        if inp_shape != self.output_shape:\n",
    "            raise RuntimeError('Unexpected inputs shape ({}, but expecting {})'\n",
    "                               .format(inp_shape, self.output_shape))\n",
    "        return inputs.reshape(*batch_dims, *self.input_shape)\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the elementwise determinant of the log Jacobian, i.e.\n",
    "        log(abs([dy_0/dx_0, ..., dy_{N-1}/dx_{N-1}])). Note that this type of\n",
    "        transform is not autoregressive, so the log Jacobian is not the sum of the\n",
    "        previous expression. However, it turns out it's always 0 (since the\n",
    "        determinant is -1 or +1), and so returning a vector of zeros works.\n",
    "        \"\"\"\n",
    "\n",
    "        log_abs_det_jacobian = torch.zeros(x.size()[:-3], dtype=x.dtype, layout=x.layout, device=x.device)\n",
    "        return log_abs_det_jacobian "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715c542",
   "metadata": {},
   "source": [
    "#### Transpose Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeTransform(Transform):\n",
    "    \"\"\"\n",
    "    A bijection that reorders the input dimensions, that is, multiplies the input by\n",
    "    a permutation matrix. This is useful in between\n",
    "    :class:`~pyro.distributions.transforms.AffineAutoregressive` transforms to\n",
    "    increase the flexibility of the resulting distribution and stabilize learning.\n",
    "    Whilst not being an autoregressive transform, the log absolute determinate of\n",
    "    the Jacobian is easily calculable as 0. Note that reordering the input dimension\n",
    "    between two layers of\n",
    "    :class:`~pyro.distributions.transforms.AffineAutoregressive` is not equivalent\n",
    "    to reordering the dimension inside the MADE networks that those IAFs use; using\n",
    "    a :class:`~pyro.distributions.transforms.Permute` transform results in a\n",
    "    distribution with more flexibility.\n",
    "    Example usage:\n",
    "    >>> from pyro.nn import AutoRegressiveNN\n",
    "    >>> from pyro.distributions.transforms import AffineAutoregressive, Permute\n",
    "    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n",
    "    >>> iaf1 = AffineAutoregressive(AutoRegressiveNN(10, [40]))\n",
    "    >>> ff = Permute(torch.randperm(10, dtype=torch.long))\n",
    "    >>> iaf2 = AffineAutoregressive(AutoRegressiveNN(10, [40]))\n",
    "    >>> flow_dist = dist.TransformedDistribution(base_dist, [iaf1, ff, iaf2])\n",
    "    >>> flow_dist.sample()  # doctest: +SKIP\n",
    "    :param permutation: a permutation ordering that is applied to the inputs.\n",
    "    :type permutation: torch.LongTensor\n",
    "    \"\"\"\n",
    "\n",
    "    domain = constraints.real\n",
    "    codomain = constraints.real\n",
    "    bijective = True\n",
    "    volume_preserving = True\n",
    "\n",
    "    def __init__(self, permutation):\n",
    "        # self.event_dim = len(permutation) \n",
    "        self.permutation = permutation\n",
    "\n",
    "        super().__init__(cache_size=1) \n",
    "\n",
    "    @lazy_property\n",
    "    def inv_permutation(self):\n",
    "        result = torch.empty_like(self.permutation, dtype=torch.long)\n",
    "        result[self.permutation] = torch.arange(self.permutation.size(0),\n",
    "                                                dtype=torch.long,\n",
    "                                                device=self.permutation.device)\n",
    "        return result\n",
    "\n",
    "    def _call(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input into the bijection\n",
    "        :type x: torch.Tensor\n",
    "        Invokes the bijection x=>y; in the prototypical context of a\n",
    "        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n",
    "        the base distribution (or the output of a previous transform)\n",
    "        \"\"\"\n",
    "\n",
    "        *batch_dims, c, h, w = x.size()\n",
    "        num_batch = len(batch_dims)\n",
    "\n",
    "        return x.permute(*np.arange(num_batch), *(self.permutation + num_batch)).contiguous()\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        \"\"\"\n",
    "        :param y: the output of the bijection\n",
    "        :type y: torch.Tensor\n",
    "        Inverts y => x.\n",
    "        \"\"\"\n",
    "\n",
    "        *batch_dims, c, h, w = y.size()\n",
    "        num_batch = len(batch_dims)\n",
    "\n",
    "        return y.permute(*np.arange(num_batch), *(self.inv_permutation + num_batch)).contiguous()\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the elementwise determinant of the log Jacobian, i.e.\n",
    "        log(abs([dy_0/dx_0, ..., dy_{N-1}/dx_{N-1}])). Note that this type of\n",
    "        transform is not autoregressive, so the log Jacobian is not the sum of the\n",
    "        previous expression. However, it turns out it's always 0 (since the\n",
    "        determinant is -1 or +1), and so returning a vector of zeros works.\n",
    "        \"\"\"\n",
    "\n",
    "        log_abs_det_jacobian = torch.zeros(x.size()[:-3], dtype=x.dtype, layout=x.layout, device=x.device)\n",
    "        return log_abs_det_jacobian\n",
    "\n",
    "\n",
    "class LearnedAffineTransform(TransformModule, transforms.AffineTransform):\n",
    "    def __init__(self, loc=None, scale=None, **kwargs):\n",
    "\n",
    "        super().__init__(loc=loc, scale=scale, **kwargs)\n",
    "\n",
    "        if loc is None:\n",
    "            self.loc = torch.nn.Parameter(torch.zeros([1, ]))\n",
    "        if scale is None:\n",
    "            self.scale = torch.nn.Parameter(torch.ones([1, ]))\n",
    "\n",
    "    def _broadcast(self, val):\n",
    "        dim_extension = tuple(1 for _ in range(val.dim() - 1))\n",
    "        loc = self.loc.view(-1, *dim_extension)\n",
    "        scale = self.scale.view(-1, *dim_extension)\n",
    "\n",
    "        return loc, scale\n",
    "\n",
    "    def _call(self, x):\n",
    "        loc, scale = self._broadcast(x)\n",
    "\n",
    "        return loc + scale * x\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        loc, scale = self._broadcast(y)\n",
    "        return (y - loc) / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18382ff2",
   "metadata": {},
   "source": [
    "#### Conditional Affine Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalAffineTransform(ConditionalTransformModule):\n",
    "    def __init__(self, context_nn, event_dim=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.event_dim = event_dim\n",
    "        self.context_nn = context_nn\n",
    "\n",
    "    def condition(self, context):\n",
    "        loc, log_scale = self.context_nn(context)\n",
    "        scale = torch.exp(log_scale)\n",
    "\n",
    "        ac = transforms.AffineTransform(loc, scale, event_dim=self.event_dim)\n",
    "        return ac\n",
    "\n",
    "\n",
    "class ActNorm(TransformModule):\n",
    "    codomain = constraints.real\n",
    "    bijective = True\n",
    "    event_dim = 3\n",
    "\n",
    "    def __init__(self, features):\n",
    "        \"\"\"\n",
    "        Transform that performs activation normalization. Works for 2D and 4D inputs. For 4D\n",
    "        inputs (images) normalization is performed per-channel, assuming BxCxHxW input shape.\n",
    "        Reference:\n",
    "        > D. Kingma et. al., Glow: Generative flow with invertible 1x1 convolutions, NeurIPS 2018.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.initialized = False\n",
    "        self.log_scale = nn.Parameter(torch.zeros(features))\n",
    "        self.shift = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    @property\n",
    "    def scale(self):\n",
    "        return torch.exp(self.log_scale)\n",
    "\n",
    "    def _broadcastable_scale_shift(self, inputs):\n",
    "        if inputs.dim() == 4:\n",
    "            return self.scale.view(1, -1, 1, 1), self.shift.view(1, -1, 1, 1)\n",
    "        else:\n",
    "            return self.scale.view(1, -1), self.shift.view(1, -1)\n",
    "\n",
    "    def _call(self, x):\n",
    "        if x.dim() not in [2, 4]:\n",
    "            raise ValueError(\"Expecting inputs to be a 2D or a 4D tensor.\")\n",
    "\n",
    "        if self.training and not self.initialized:\n",
    "            self._initialize(x)\n",
    "\n",
    "        scale, shift = self._broadcastable_scale_shift(x)\n",
    "        outputs = scale * x + shift\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        if y.dim() not in [2, 4]:\n",
    "            raise ValueError(\"Expecting inputs to be a 2D or a 4D tensor.\")\n",
    "\n",
    "        scale, shift = self._broadcastable_scale_shift(y)\n",
    "        outputs = (y - shift) / scale\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the elementwise determinant of the log Jacobian, i.e.\n",
    "        log(abs([dy_0/dx_0, ..., dy_{N-1}/dx_{N-1}])). Note that this type of\n",
    "        transform is not autoregressive, so the log Jacobian is not the sum of the\n",
    "        previous expression. However, it turns out it's always 0 (since the\n",
    "        determinant is -1 or +1), and so returning a vector of zeros works.\n",
    "        \"\"\"\n",
    "\n",
    "        ones = torch.ones(x.shape[0], device=x.device)\n",
    "        if x.dim() == 4:\n",
    "            _, _, h, w = x.shape\n",
    "            log_abs_det_jacobian = h * w * torch.sum(self.log_scale) * ones\n",
    "        else:\n",
    "            log_abs_det_jacobian = torch.sum(self.log_scale) * ones\n",
    "\n",
    "        return log_abs_det_jacobian\n",
    "\n",
    "    def _initialize(self, inputs):\n",
    "        \"\"\"Data-dependent initialization, s.t. post-actnorm activations have zero mean and unit\n",
    "        variance. \"\"\"\n",
    "        if inputs.dim() == 4:\n",
    "            num_channels = inputs.shape[1]\n",
    "            inputs = inputs.permute(0, 2, 3, 1).reshape(-1, num_channels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            std = inputs.std(dim=0)\n",
    "            mu = (inputs / std).mean(dim=0)\n",
    "            self.log_scale.data = -torch.log(std)\n",
    "            self.shift.data = -mu\n",
    "\n",
    "        self.initialized = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a06f89",
   "metadata": {
    "id": "47a06f89"
   },
   "source": [
    "#### Image transforms architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c85a88b",
   "metadata": {
    "id": "9c85a88b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "class BasicFlowConvNet(nn.Module):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, param_dims, context_dims: int = None, param_nonlinearities=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.param_dims = param_dims\n",
    "        self.count_params = len(param_dims)\n",
    "        self.output_dims = sum(param_dims)\n",
    "\n",
    "        self.context_dims = context_dims\n",
    "        self.param_nonlinearities = param_nonlinearities\n",
    "\n",
    "        self.seq1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + context_dims if context_dims is not None else in_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, self.output_dims, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        ends = torch.cumsum(torch.tensor(param_dims), dim=0)\n",
    "        starts = torch.cat((torch.zeros(1).type_as(ends), ends[:-1]))\n",
    "        self.param_slices = [slice(s.item(), e.item()) for s, e in zip(starts, ends)]\n",
    "\n",
    "        def weights_init(m):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, 0., 1e-4)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, inputs, context=None):\n",
    "        # pyro affine coupling splits on the last dimenion and not the channel dimension\n",
    "        # -> we want to permute the dimensions to move the last dimension into the channel dimension\n",
    "        # and then transpose back\n",
    "\n",
    "        if not ((self.context_dims is None) == (context is None)):\n",
    "            raise ValueError('Given context does not match context dims: context: {} and context_dims:{}'.format(context, self.context_dims))\n",
    "\n",
    "        *batch_dims, h, w, c = inputs.size() \n",
    "        num_batch = len(batch_dims)\n",
    "\n",
    "        permutation = np.array((2, 0, 1)) + num_batch\n",
    "        outputs = inputs.permute(*np.arange(num_batch), *permutation).contiguous() \n",
    "\n",
    "        if context is not None:\n",
    "            # assuming scalar inputs [B, C] \n",
    "            \"\"\"\n",
    "            torch.Size([16, 16, 8])\n",
    "            torch.Size([1, 1, 16])\n",
    "            Ouptus torch.Size([1, 8, 16, 16])\n",
    "            \"\"\"\n",
    "            context = context.view(*context.shape, 1, 1).expand(-1, *outputs.shape[1:]) \n",
    "            outputs = torch.cat([outputs, context], 0)\n",
    "            if num_batch == 0: \n",
    "                outputs = outputs.view(-1, *outputs.shape) \n",
    "\n",
    "        outputs = self.seq1(outputs) \n",
    "        if num_batch == 0: \n",
    "            outputs = outputs.view(*outputs.shape[1:]) \n",
    "\n",
    "        permutation = np.array((1, 2, 0)) + num_batch\n",
    "        outputs = outputs.permute(*np.arange(num_batch), *permutation).contiguous()\n",
    "\n",
    "        if self.count_params > 1:\n",
    "            outputs = tuple(outputs[..., s] for s in self.param_slices)\n",
    "\n",
    "        if self.param_nonlinearities is not None:\n",
    "            if isinstance(self.param_nonlinearities, Iterable):\n",
    "                outputs = tuple(n(o) for o, n in zip(outputs, self.param_nonlinearities))\n",
    "            else:\n",
    "                outputs = tuple(self.param_nonlinearities(o) for o in outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5beb1c",
   "metadata": {
    "id": "9c5beb1c"
   },
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35de283",
   "metadata": {
    "id": "e35de283"
   },
   "source": [
    "**Structural Causal Model**\n",
    "\n",
    "($e_g$, $e_x$) ~ N(0, 1)\n",
    "\n",
    "g ~ Bern($e_g$)\n",
    "\n",
    "X ~ $F(e_x, g)$\n",
    "\n",
    "![SCM](./images/scm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5f7a8",
   "metadata": {
    "id": "e9c5f7a8"
   },
   "source": [
    "![Full Model](./images/x_transforms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392accc",
   "metadata": {
    "id": "8392accc"
   },
   "source": [
    "**Multi-Scale Architecture**\n",
    "\n",
    "Disadvantage of normalizing flows is that they operate on the exact same dimensions as the input.\n",
    "    \n",
    "    If the input is high-dimensional, so is the latent space,  will requires larger computational cost to learn suitable transformations. \n",
    "    \n",
    "    However, in image domain, many pixels contain less information. \n",
    "\n",
    "Multi-scale architecture : After the first 𝑁  flow transformations, we split off half of the latent dimensions and directly evaluate them on the prior. The other half is run through  𝑁  more flow transformations, and depending on the size of the input, we split it again in half or stop overall at this position. \n",
    "\n",
    "    Squeeze and split:\n",
    "\n",
    "![Squeeze and Split](./images/squeeze_split.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2e2db50",
   "metadata": {
    "id": "b2e2db50"
   },
   "outputs": [],
   "source": [
    "class FlowSCM(PyroModule):\n",
    "    def __init__(self, use_affine_ex=True):\n",
    "        super().__init__() \n",
    "        self.num_scales = 2\n",
    "        self.flows_per_scale = 1\n",
    "        self.use_actnorm = False \n",
    "        self.use_affine_ex = use_affine_ex \n",
    "\n",
    "        self.register_buffer(\"glasses_base_loc\", torch.zeros([1, ], requires_grad=False))\n",
    "        self.register_buffer(\"glasses_base_scale\", torch.ones([1, ], requires_grad=False))\n",
    "\n",
    "        self.register_buffer(\"glasses_flow_lognorm_loc\", torch.zeros([], requires_grad=False))\n",
    "        self.register_buffer(\"glasses_flow_lognorm_scale\", torch.ones([], requires_grad=False))\n",
    "\n",
    "        self.glasses_flow_lognorm = AffineTransform(loc=self.glasses_flow_lognorm_loc.item(), scale=self.glasses_flow_lognorm_scale.item())\n",
    "\n",
    "        self.glasses_flow_components = ComposeTransformModule([Spline(1)])\n",
    "        self.glasses_flow_constraint_transforms = ComposeTransform([self.glasses_flow_lognorm,\n",
    "            SigmoidTransform()])\n",
    "        self.glasses_flow_transforms = ComposeTransform([self.glasses_flow_components,\n",
    "            self.glasses_flow_constraint_transforms])\n",
    "\n",
    "        glasses_base_dist = Normal(self.glasses_base_loc, self.glasses_base_scale).to_event(1)\n",
    "        self.glasses_dist = TransformedDistribution(glasses_base_dist, self.glasses_flow_transforms)\n",
    "        \n",
    "        self._build_image_flow()\n",
    "        self.register_buffer(\"x_base_loc\", torch.zeros([1, 64, 64], requires_grad=False))\n",
    "        self.register_buffer(\"x_base_scale\", torch.ones([1, 64, 64], requires_grad=False))\n",
    "        self.x_base_dist = Normal(self.x_base_loc, self.x_base_scale).to_event(3)\n",
    "\n",
    "    def model(self): \n",
    "        glasses_ = pyro.sample(\"glasses_\", self.glasses_dist)\n",
    "        glasses = pyro.sample(\"glasses\", dist.Bernoulli(glasses_))\n",
    "        glasses_context = self.glasses_flow_constraint_transforms.inv(glasses_)\n",
    "\n",
    "        cond_x_transforms = ComposeTransform(\n",
    "            ConditionalTransformedDistribution(self.x_base_dist, self.x_transforms)\n",
    "            .condition(glasses_context).transforms\n",
    "            ).inv\n",
    "        cond_x_dist = TransformedDistribution(self.x_base_dist, cond_x_transforms)\n",
    "        x = pyro.sample(\"x\", cond_x_dist)\n",
    "\n",
    "        return x, glasses \n",
    "\n",
    "\n",
    "    def sample(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.model()\n",
    "\n",
    "        return (*samples,)\n",
    "\n",
    "\n",
    "    def _build_image_flow(self):\n",
    "        self.trans_modules = ComposeTransformModule([])\n",
    "        self.x_transforms = [] \n",
    "        self.hidden_channels = 3 \n",
    "\n",
    "        c = 1\n",
    "        for _ in range(self.num_scales):\n",
    "            self.x_transforms.append(SqueezeTransform())\n",
    "            c *= 4\n",
    "\n",
    "            for _ in range(self.flows_per_scale):\n",
    "                if self.use_actnorm:\n",
    "                    actnorm = ActNorm(c)\n",
    "                    self.trans_modules.append(actnorm)\n",
    "                    self.x_transforms.append(actnorm)\n",
    "\n",
    "                gcp = GeneralizedChannelPermute(channels=c)\n",
    "                self.trans_modules.append(gcp)\n",
    "                self.x_transforms.append(gcp)\n",
    "\n",
    "                self.x_transforms.append(TransposeTransform(permutation=torch.tensor((1, 2, 0))))\n",
    "\n",
    "                ac = ConditionalAffineCoupling(c // 2, BasicFlowConvNet(c // 2, self.hidden_channels, (c // 2, c // 2), 1))\n",
    "                self.trans_modules.append(ac)\n",
    "                self.x_transforms.append(ac)\n",
    "\n",
    "                self.x_transforms.append(TransposeTransform(torch.tensor((2, 0, 1))))\n",
    "\n",
    "            gcp = GeneralizedChannelPermute(channels=c)\n",
    "            self.trans_modules.append(gcp)\n",
    "            self.x_transforms.append(gcp)\n",
    "\n",
    "        self.x_transforms += [\n",
    "            ReshapeTransform((4**self.num_scales, 64 // 2**self.num_scales, 64 // 2**self.num_scales), (1, 64, 64))\n",
    "        ]\n",
    "\n",
    "        if self.use_affine_ex:\n",
    "            affine_net = DenseNN(1, [16, 16], param_dims=[1, 1])\n",
    "            affine_trans = ConditionalAffineTransform(context_nn=affine_net, event_dim=3)\n",
    "\n",
    "            self.trans_modules.append(affine_trans)\n",
    "            self.x_transforms.append(affine_trans) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d322f0",
   "metadata": {
    "id": "28d322f0"
   },
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67699d",
   "metadata": {
    "id": "0f67699d"
   },
   "source": [
    "**Bits Per Dimension (BPD)**\n",
    "\n",
    "As a final piece for calculating our loss function we use a concept called Bits Per Dimension (BPD)\n",
    "\n",
    "This loss calculates the number of bits needed to represent some sample x’ in our distribution P(X), with less bits corresponding to a larger likelihood\n",
    "\n",
    "We change the base of the log likelihood to base 2 and then divide by the product over the dimensions of our image (which is the width and height)\n",
    "\n",
    "Essentially we normalize over the dimensions we have over our images to allow for comparison between images of varying resolutions \n",
    "\n",
    "    This is important as we change the image resolutions to help speed up training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b42db9c0",
   "metadata": {
    "id": "b42db9c0"
   },
   "outputs": [],
   "source": [
    "class Experiment(pl.LightningModule): \n",
    "    def __init__(self, model): \n",
    "        super().__init__() \n",
    "        self.flow_model = model \n",
    "        self.model = flow_model.model \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        glasses_params = self.flow_model.glasses_flow_components.parameters()\n",
    "        x_params = self.flow_model.trans_modules.parameters() \n",
    "\n",
    "        optimizer =  torch.optim.Adam([\n",
    "            {'params': x_params, 'lr': 1e-3},\n",
    "            {'params': glasses_params, 'lr': 1e-3},\n",
    "        ], lr=1e-3, eps=1e-5) \n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def _loss(self, **batch): \n",
    "        cond_model = pyro.condition(self.flow_model.sample, data=batch) \n",
    "        model_trace = pyro.poutine.trace(cond_model).get_trace(batch[\"x\"].shape[0]) \n",
    "        model_trace.compute_log_prob() \n",
    "\n",
    "        log_probs = {}\n",
    "        nats_per_dim = {}\n",
    "        for name, site in model_trace.nodes.items():\n",
    "            if site[\"type\"] == \"sample\" and site[\"is_observed\"]:\n",
    "                log_probs[name] = site[\"log_prob\"].mean()\n",
    "                log_prob_shape = site[\"log_prob\"].shape\n",
    "                value_shape = site[\"value\"].shape\n",
    "                if len(log_prob_shape) < len(value_shape):\n",
    "                    dims = np.prod(value_shape[len(log_prob_shape):])\n",
    "                else:\n",
    "                    dims = 1.\n",
    "                nats_per_dim[name] = -site[\"log_prob\"].mean() / dims\n",
    "                if self.hparams.validate:\n",
    "                    print(f'at site {name} with dim {dims} and nats: {nats_per_dim[name]} and logprob: {log_probs[name]}')\n",
    "                    if torch.any(torch.isnan(nats_per_dim[name])):\n",
    "                        raise ValueError('got nan')\n",
    "\n",
    "        return log_probs, nats_per_dim \n",
    "\n",
    "\n",
    "    def prep_batch(self, batch):\n",
    "        x = batch[0].float() \n",
    "        context = batch[1][:,2] \n",
    "        return {\"x\": x, \"glasses\": context} \n",
    "\n",
    "\n",
    "    def training_step(self, batch, *args): \n",
    "        batch = self.prep_batch(batch) \n",
    "        log_probs, nats_per_dim = self._loss(**batch) \n",
    "        loss = torch.stack(tuple(nats_per_dim.values())).sum() \n",
    "\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34b8f6",
   "metadata": {
    "id": "6c34b8f6"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(default_root_dir=\"./\", \n",
    "                         gpus=0 if torch.cuda.is_available() else 0, \n",
    "                         max_epochs=100, \n",
    "                         gradient_clip_val=1.0,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_bpd\"),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "trainer.logger._log_graph = True\n",
    "trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "train_data_loader = data.DataLoader(cartoon_data, batch_size=64, shuffle=True, drop_last=True, pin_memory=True, num_workers=8)\n",
    "result = None \n",
    "\n",
    "flow_model = FlowSCM() \n",
    "exp = Experiment(flow_model)\n",
    "trainer.fit(exp, train_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7f05669",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7f05669",
    "outputId": "7ec5bf75-0595-438f-d893-d4a14e746e74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SqueezeTransform(),\n",
       " GeneralizedChannelPermute(),\n",
       " TransposeTransform(),\n",
       " ConditionalAffineCoupling(\n",
       "   (nn): BasicFlowConvNet(\n",
       "     (seq1): Sequential(\n",
       "       (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): ReLU()\n",
       "       (2): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "       (3): ReLU()\n",
       "       (4): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " TransposeTransform(),\n",
       " GeneralizedChannelPermute(),\n",
       " SqueezeTransform(),\n",
       " GeneralizedChannelPermute(),\n",
       " TransposeTransform(),\n",
       " ConditionalAffineCoupling(\n",
       "   (nn): BasicFlowConvNet(\n",
       "     (seq1): Sequential(\n",
       "       (0): Conv2d(9, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): ReLU()\n",
       "       (2): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "       (3): ReLU()\n",
       "       (4): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " TransposeTransform(),\n",
       " GeneralizedChannelPermute(),\n",
       " ReshapeTransform(),\n",
       " ConditionalAffineTransform(\n",
       "   (context_nn): DenseNN(\n",
       "     (layers): ModuleList(\n",
       "       (0): Linear(in_features=1, out_features=16, bias=True)\n",
       "       (1): Linear(in_features=16, out_features=16, bias=True)\n",
       "       (2): Linear(in_features=16, out_features=2, bias=True)\n",
       "     )\n",
       "     (f): ReLU()\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_model.x_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008efcf1",
   "metadata": {
    "id": "008efcf1"
   },
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4275a",
   "metadata": {
    "id": "6ab4275a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8e68549",
   "metadata": {
    "id": "d8e68549"
   },
   "source": [
    "**References**\n",
    "\n",
    "1. Pawlowski, N., Castro, D. C., & Glocker, B. (2020). Deep structural causal models for tractable counterfactual inference. arXiv preprint arXiv:2006.06485.\n",
    "2. Normalizing Flows - Introduction (Part 1) — Pyro Tutorials 1.7.0 documentation\n",
    "3. Cartoon Dataset \n",
    "4. Normalizing Flows for image modeling\n",
    "5. Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2017). “Density estimation using Real NVP,” In: 5th International Conference on Learning Representations, ICLR 2017.\n",
    "6. Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. (2019). “Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design,” in Proceedings of the 36th International Conference on Machine Learning, vol. 97, pp. 2722–2730\n",
    "7. Flow-based Deep Generative Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc93ac",
   "metadata": {
    "id": "fcbc93ac"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS7290-normalizing-flows-SCM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
